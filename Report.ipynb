{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Report",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtsyang/dl-reproducibility-project/blob/master/Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJKQLTZVTrE_",
        "colab_type": "text"
      },
      "source": [
        "# Striving for Simplicity: The All Convolutional Net\n",
        "###### *Wei-Tse Yang, Sunwei Wang, and Qingyuan Cao*\n",
        "###### 14/4/2020\n",
        "In this notebook, we try to reproduce the TABLE 3 in [original paper](https://arxiv.org/abs/1412.6806). The source code of one of the models on ```Pytorch``` and the training procedure on Google Colab can be found in [github](https://github.com/StefOe/all-conv-pytorch). **We adopt the original training procedure and change it to the Python class. Also, we build the models from scratch on Pytorch.**\n",
        "\n",
        "---\n",
        "\n",
        "# Brief Introduction\n",
        "The paper shows that replacing the max-pooling with the convolutional with increased strides can improve the performance. The authors prove it by training models with max-pooling, models removing max-pooling, and models replacing max-pooling with convolutional layers. The results show the models replacing max-pooling with convolutional layers with strides generally have better performance. We provide the detail explanation as follows. The authors tested 12 networks by designing 3 model bases and 3 branches. \n",
        "\n",
        "## *Base: Model A, Model B, and Model C*\n",
        "Since the design of convolutional layers would influence the performance, the authors test three model bases. **Model A** uses the 5x5 strides. **Model B** uses the 5x5 strides but also adds one convolutional layer with 1x1 strides after that. **Model C** uses two convolutional layers with 3x3 strides. \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1HKDGWePX-PkBqRbeb8J_mwO-DDULhU2q' width=\"600px\"/>\n",
        "\n",
        "\n",
        "## *Branch: Model, Strided-CNN, ConvPool-CNN, and ALL-CNN*\n",
        "Each model base has one original model and thee branches. **“Model”** is the model with max-pooling. **“Strided-CNN”** is the model removing max-pooling. **“All-CNN”** is the model replacing max-pooling with convolutional strides. The better performance of “All-CNN” might result from more parameters than “Model” and “Strided-CNN”. To solve it, “ConvPool-CNN” is proposed. **“ConvPool-CNN”** is the model with max-pooling and one more convolutional layer before the pooling. “ConvPool-CNN” should have the same number of parameters as “All-CNN”. Therefore, if “All-CNN” has a better performance than “ConvPool-CNN”, we can prove the better performance on “All-CNN” does not result from more parameters. \n",
        "We show architecture with the base of model C in the following image. \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1gzpTwoW_Xx8YrHZdvU0ZmFT3n-1ktx1v' width=\"600px\"/>\n",
        "\n",
        "---\n",
        "# Experiment Setup \n",
        "All 12 networks are trained on the CIFAR-10 with the stochastic gradient descent with a fixed momentum of 0.9 and 350 epochs. The learning rate γ is chosen from the set ∈ [0.25, 0.1, 0.05, 0.01]. It is also scheduled by multiplying with a fixed factor of 0.1 in the epoch S= [200, 250, 300]. The paper only presents the best performance among all learning rates. Based on the source code, the performance is directly evaluated on the CIFAR-10 test set. In other words, **the source code did not use the cross-validation for hyper-parameter tuning!** \n",
        "\n",
        "---\n",
        "\n",
        "# Reproduction Results\n",
        "\n",
        "\n",
        "| Model      |  Error Rate of Paper  | Error Rate of Ours| \n",
        "|-|-|-|\n",
        "|   Model A   | 12.47%|19.27%|\n",
        "|   Strided-CNN-A  |13.46% |20.27%|\n",
        "|   **ConvPool-CNN-A**  |**10.21%**|**15.46%**|\n",
        "|   ALL-CNN-A  |10.30% |15.60%|\n",
        "| | ||\n",
        "|   Model B | 10.20%|**17.01%**|\n",
        "|   Strided-CNN-B  | 10.98%|23.20%|\n",
        "|   ConvPool-CNN-B  | 9.33%|18.22%|\n",
        "|   ALL-CNN-B   | ***9.10%***|*29.48%|\n",
        "| | ||\n",
        "|   Model C  | 9.74%|**13.07%**|\n",
        "|   Strided-CNN-C  | 10.19%|15.49%|\n",
        "|   ConvPool-CNN-C  | 9.31%|14.39%|\n",
        "|   ALL-CNN-C  | ***9.08%***|17.89%|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsojPAB1q75j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The example of training procedure\n",
        "# Choose the model A \n",
        "training=Training(baseModel=[True,False,False])\n",
        "# Create the dataset\n",
        "training.createDataset()\n",
        "# Choose the branch: All-CNN\n",
        "training.modifiedModel=[False,False,False,True]\n",
        "# Start Training\n",
        "training.Procedure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVksKoMlaxi6",
        "colab_type": "text"
      },
      "source": [
        "# Cross-Valudation \n",
        "Since the source code did not use the validation set for hyper-parameters tuning, we conduct the cross-validation in this section. We split 5% of the training data to create the cross-validation set. We show the results in the following table. The performance on the test set does not have too much difference from the counterpart on the validation set. However, if we compare the test error with the original test error, the test error with cross-validation is generally higher. This is because right now we save the best model based on the validation error, and then evaluate the test set with it. \n",
        "\n",
        "| Model     |  Validation Set Error | Test Error| Original Test Error|\n",
        "|-|-|-|-|\n",
        "|   Model A   | 21.20% |20.45%|19.27%|\n",
        "|   Strided-CNN-A  |21.72% |21.38%|20.27%|\n",
        "|   **ConvPool-CNN-A**  |?|?|**15.46%**|\n",
        "|   ALL-CNN-A  |? |?|15.60%|\n",
        "| | ||\n",
        "|   Model B | 16.65%|17.81%|**17.01%**|\n",
        "|   Strided-CNN-B  | 17.53%|18.68%|23.20%|\n",
        "|   ConvPool-CNN-B  | 17.53%|17.51%|18.22%|\n",
        "|   ALL-CNN-B   | *24.53% | *25.78%|*29.48%|\n",
        "| | ||\n",
        "|   **Model C**  | **14.13%**|**14.87%**|**13.07%**|\n",
        "|   Strided-CNN-C  | 20.89%|21.67%| 15.49%| \n",
        "|   ConvPool-CNN-C  | 17.81%|17.60%| 14.39%|\n",
        "|   ALL-CNN-C  | ?|?| 17.89%|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zAdzu2mq8l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross Validation can be conducted by settting validation equal to True \n",
        "training=Training(validation=True,bestModel_allLR=True,baseModel=[True,False,False])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hztDzk-5qvm7",
        "colab_type": "text"
      },
      "source": [
        "# DropOut and Batch Normalization\n",
        "Dropout is a simple method to prevent neural networks from overfitting. In our all convolutional net paper, the author stated that dropout was used to regularize all networks. \n",
        "Dropout was almost essential in all the state-of-the-art networks before the introduction of batch normalization(BN). With the introduction of BN, it has shown its effectiveness and practicability in the recent neural networks. However, there is evidence that when these two techniques are used combinedly in a network, the performance of the network actually becomes worse. (Ioffe & Szegedy, 2015). In our study, we will investigate the results using BN and Dropout independently and also the effect of equipping both BN and Dropout simultaneously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZXK-gvLJ5np",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### BatchNorm *only*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh2N6fpLq9Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(dropOut=False, BN=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z695JJpVnmkh",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=17zb3ZUMTgRVLyRa1HYKm1SXqd-M_b6ai' width=\"500px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHxeNuDgKOkp",
        "colab_type": "text"
      },
      "source": [
        "### BatchNorm with Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCcecSDaKXT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(dropOut=True, BN=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP4P4SoCLLwD",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=11rOzypoJjhbfdQsD6KND2SyqtLfOuh2n' width=\"500px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb10QUPVfAI-",
        "colab_type": "text"
      },
      "source": [
        "### Dropout *only*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GjPD1UqfKnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(dropOut=True, BN=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-41_3aFfK5Z",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1eVqvPxWPCAovkP3lNT1G_odgyisbAlLr' width=\"500px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5uNJZc_LtTc",
        "colab_type": "text"
      },
      "source": [
        "### Without using BatchNorm or Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H7CEUUbL914",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(dropOut=False, BN=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAqeF0EIMD7E",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1uzW63lONO39_Sy9JGhzLc6Qqc976qe00' width=\"500px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVvoCDCwgBrq",
        "colab_type": "text"
      },
      "source": [
        "We compare the results of different combination of these two techniques and generated the table below: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kegTz4wwVQm8",
        "colab_type": "text"
      },
      "source": [
        "| Model     |  BN only | BN + Dropout |Dropout only| No BN no Dropout |\n",
        "|-|-|-|-|-|\n",
        "|   Model A   | no converge |no converge|19.27%| 14%|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr4OCR_QZf5z",
        "colab_type": "text"
      },
      "source": [
        "As shown in the table above, we used general Model A to study the two techniques Batch Normalziation (BN) and Dropout, and whether it increase or decrease our model performance in this case. We implemented BN layer between two convolution layers, right before feeding into ReLu actiations. Dropout is also applied in between convolution layers, and it is used after pooling layer. \n",
        "The original paper stated that they used Dropout only, and we found out that that using BN without Dropout or combining both BN with Dropout will not let our model converge. Li, Xiang, et al. (2019) stated in their papers that the worse performance may be the result of variance shifts. However, using Dropout only does lead the model to converge, giving the result of 19.27%. But the performance is still not as good as 14% without using either BN or dropout. It might be due to that we did not have the time to tune hyperparameter, dropout rate, we only used the parameter from the original paper: 20% for dropping out inputs and 50% otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmXwCYC2qhCF",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer\n",
        "The default optimizer used in the paper and in our reproduction is Stochastic Gradient Descent (SGD) with momentum. We experimented with different optimizers, since Adaptive Moment Estimation (Adam) is one of the most popular optimization algorithms, we decided to run Adam instead of SGD optimizer in our model. However, the model did not converge with Adam under the specific setting that we tried to reproduce. So even though in theory Adam combines the advantage of two SGD variants, RMSProp and AdaGrad, it is not very consistent in our specific setting to converge to an optimal solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pvxevz9n1DI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Default SGD optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L24H7bgEq9kD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG6UzCsfoEsu",
        "colab_type": "text"
      },
      "source": [
        "# Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOx0yCttcaFv",
        "colab_type": "text"
      },
      "source": [
        "# Reference\n",
        "\n",
        "Springenberg, Jost Tobias, et al. \"Striving for simplicity: The all convolutional net.\" arXiv preprint arXiv:1412.6806 (2014).\n",
        "\n",
        "\n",
        "Li, Xiang, et al. \"Understanding the disharmony between dropout and batch normalization by variance shift.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2019).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "982dZGSNat4a",
        "colab_type": "text"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "\n",
        "*   [Github Repository](https://github.com/wtsyang/dl-reproducibility-project)\n",
        "*   [A Python Class to build all Models](https://github.com/wtsyang/dl-reproducibility-project/blob/master/Model.py)\n",
        "*   [A Python Class for Training Procedure](https://github.com/wtsyang/dl-reproducibility-project/blob/master/Training.py)\n",
        "*   [The Notebooks for Model A](https://github.com/wtsyang/dl-reproducibility-project/tree/master/modelA)\n",
        "*   [The Notebooks for Model B](https://github.com/wtsyang/dl-reproducibility-project/tree/master/modelB)\n",
        "*   [The Notebooks for Model C](https://github.com/wtsyang/dl-reproducibility-project/tree/master/modelC)\n",
        "*   [The Notebooks for Cross Validation](https://github.com/wtsyang/dl-reproducibility-project/tree/master/crossValidation)\n",
        "*   [The Notebooks for DropOut and Batch Normalization](https://github.com/wtsyang/dl-reproducibility-project/tree/master/BN_Dropout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}